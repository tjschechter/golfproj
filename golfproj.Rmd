---
title: 'Golf Machine Learning Project: Predicting Successful Players in the PGA Tour
  with TidyModels'
author: "Thomas Schechter and Ian Bogley"
date: "12/10/2021"
output:
  html_document:
    df_print: paged
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE,message=FALSE)
```

Golf is a sport with diverse and detailed statistics, making it ideal grounds for prediction and analysis. The purpose of this project will be to use machine learning algorithms to predict whether a player will be successful or not. To do so, we will use data from the PGA 2017 Tour.

In particular, our question will be specified as follows: What players will finish in the top 10 of an event at least once? (Is this accurate phrasing? Is this a good question? May reduce variability of our outcome variable by quite a bit in comparison to predicting number of top 10 finishes. Number of players who never managed to break into the top 10 are 34, while the number who did at some point is 161. This means that with a k-fold cross-validation approach where k=5, the probability of not having a False observation in one of the 5 folds is:[I forgot my probability stuff, but I think we might have a problem here.])

To start, let's begin by reading in the data and install necessary packages.
```{r intro}
#Package unloading
library(pacman)

p_load(tidyverse,data.table,tidymodels,stargazer,ggpubr,
       janitor,sandwich,xtable,parsnip, caret, randomForest, doParallel)


#Read in data
pga_data17 <- fread("PGATOUR_data2.csv")
```

Now, we will clean the data, subsetting to remove null and NA values.

```{r cleaning,results='asis'}
#Subset data
pga_data <- pga_data17[1:195,] %>% clean_names() %>%
  mutate(top_ten_finisher = as.factor(number_of_top_tens>0))

#check for further na values
lapply(pga_data,FUN = function(x) {sum(is.na(x))}) %>%
  as.data.frame() %>% melt() %>% 
  rename("Var" = "variable","NAs"="value") %>%
  filter(NAs>0) %>% xtable()

#There is one missing value in the "points behind lead" column, we will use the mean value to impute
pga_data$points_behind_lead[33] <- mean(pga_data$points_behind_lead,na.rm = T)

#Several variables seem to be character variables as well
pga_data$fairways_hit <- gsub(",","",pga_data$fairways_hit) %>% 
  as.numeric()
pga_data$total_drives <- gsub(",","",pga_data$total_drives) %>%
  as.numeric()
```

Now, let's graph potential factors in top ten finishes and look for correlations.


```{r EDA}
#Begin exploring the relationships in the data

#Correlation b/w rounds played and top tens (Is this looking for issues in representation? Players with top tens more represented, so perhaps those without top tens are underrepresented?)
model1 <- lm(rounds_played~number_of_top_tens, pga_data)

```

```{r table1,echo=FALSE,results='asis'}
stargazer(model1)
```


```{r plots}
#Plot the rounds played relationship
round_plot <-  ggplot(pga_data, aes(rounds_played, number_of_top_tens)) + 
  geom_point() +  geom_smooth(method = "lm") +
  labs(x = "Rounds Played",
       y = "Number of Top Tens")

#Plotting strokes gained putting vs. top ten finishes
putt_plot <- ggplot(pga_data, aes(sg_putting_per_round, number_of_top_tens)) +
  geom_point() +  geom_smooth(method = "lm") +
  labs(x = "Shots Gained Putting per Round",
       y = "Number of Top Tens")

#Plotting average drive distance vs. top ten finishes
drive_plot <- ggplot(pga_data, aes(avg_driving_distance,number_of_top_tens)) +
  geom_point() + geom_smooth(method = "lm") +
  labs(x = "Avg Driving Distance",
       y = "Number of Top Tens")

#Plotting Fairway hit percentage vs top ten finishes
fairway_plot <- ggplot(pga_data, aes(x=fairway_hit_percent,y = number_of_top_tens)) +
  geom_point() + geom_smooth(method = "lm")+
  labs(x = "Fairway Hit Percentage",
       y = "Number of Top Tens")

#Plotting Shots gained off the tee vs top ten finishes
tee_plot <- ggplot(pga_data, aes(x=sg_ott,y = number_of_top_tens)) + 
  geom_point() + geom_smooth(method = "lm") +
  labs(x = "Shots Gained: Off the Tee",
       y = "Number of Top Tens")

#Arrange previous plots into a single graphic
ggarrange(round_plot,putt_plot,drive_plot,fairway_plot,tee_plot)

```
Not a significant amount of visual correlation between number of rounds played and number of top ten finishes.

Strokes gained putting seems to have more of an effect on success.

Some relationship appears present in average driving distance, though with more outliers on both ends of the spectrum.

There seems to be even less of a correlation in terms of fairway hit percentage than the previous ones. Even if there is, it is much smaller of an impact than the other factors examined.


Let's examine these relationships as linear regressions, identifying the impact of each and the robustness of the coefficients.

```{r models}
for (i in 1:5) {
  #vector of variable names
  variables <- c("rounds_played","sg_putting_per_round",
                 "avg_driving_distance","fairway_hit_percent",
                 "sg_ott")
  
  #models for each variable
  eval(parse(text = paste("model_",i+1,
                          " <- lm(number_of_top_tens ~ ",variables[i],
                          ",pga_data)",sep = "")))
  
  #robust se for each variable
  eval(parse(text = paste("robust_se_",i+1,
                          " <- list(sqrt(diag(vcovHC(model_",i+1,"))))",
                          sep ="")))
}

```
```{r table_2,echo=FALSE,results='asis'}
stargazer(model_2,model_3,model_4,model_5,model_6)
```

We can also go further, collecting the variables with the highest correlation to our dependant variable.

```{r cor}
#Correlation coefficients of each var with top ten finisher
cor_coef <- lapply(pga_data[,-c(1,4,5,70)], 
                   FUN = function(x) {
                     cor(x=as.numeric(x),
                         y = as.numeric(pga_data$top_ten_finisher))})

#get a list of the top 20 variables by absolute cor coef
top_var <- cor_coef[order(abs(unlist(cor_coef)),
                          decreasing=T)[1:20]] %>%
  as.data.frame() %>% gather() %>% 
  rename(var = "key", cor = "value")
```
```{r table_3,echo=F,results='asis'}
xtable(top_var)
```

Now we have a set of variables which are highly correlated with our explanatory variable. Now let's check for colinearity between these variables.

```{r col}
#Subset data to include only top
top_var_data <- pga_data %>% select(top_var$var)

#Create a correlation matrix of top variables
top_var_cormat <- round(cor(top_var_data),2) 

#Delete duplicate values
top_var_cormat[lower.tri(top_var_cormat)] <- NA

#Plot Collinearity values as a heatmap
top_var_cormat %>% melt() %>%
  ggplot(aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() + theme(axis.text.x= element_text(angle = -75),
                      plot.title=element_text(hjust = .5)) +
  labs(title = "Collinearity Matrix") + xlab("") + ylab("") 
```

We can see that there are quite a few variable pairings that may present a danger. Let's take the mean absolute correlation coefficient, then take a look at all the pairs of explanatory variables which have an absolute correlation coefficient above it. We will use this as a cutoff, meaning that these variable pairings are ones we want to avoid in our models.

```{r var_avoid}
#Create a df of variable pairs to avoid, 
#using mean correlation coefficient as the cutoff
top_var_avoid <- top_var_cormat %>% melt() %>% na.omit() %>%
  filter((abs(value)>abs(mean(value,na.rm=T)))&!value==1) %>%
  arrange(-abs(value))
```
For presentation purposes, we will only show the first 10 pairs.

```{r table_4,echo=FALSE,results='asis'}
top_var_avoid %>% head(10) %>% xtable()
```

When creating our models, we will need to ensure that no two variables in the same row are included in the same model. To do so, let's create a list of variable pairings that cannot be put together.

To choose between two variables, let's use the one with the largest correlation coefficient. If that one also has a potential replacement as well, we will again use the replacement.

### ```{sql, connection=db}
### ALTER TABLE "pga_data" RENAME COLUMN "SG:OTT" TO "sgott"
###```


```{r top_var_avoid}
#Merge the df of top variables by collinearity with correlation coefficients df
top_var1 <- top_var %>% rename(Var1 = var, Cor1 = cor)
top_var2 <- top_var %>% rename(Var2 = var, Cor2 = cor)
top_var_avoid_pairs <- top_var_avoid[,-3] %>% 
  left_join(top_var1) %>%
  left_join(top_var2) %>%
  mutate(choose_var1 = abs(Cor1)>abs(Cor2))

#From here, interesting tidbit. choose_var1 is always true, 
#meaning that the variable in Var1 is always to be chosen
#over the one in Var2
#So, let's replace each Var2
#With the Var1 with the largest absolute Cor1 value.
top_var_replacements <- top_var_avoid_pairs %>% group_by(Var2) %>% 
  summarise(Var1 = unique(Var1),Cor1) %>%
  group_by(Var2) %>% filter(Cor1==max(Cor1)) %>%
  .[,-3] %>% rename(variable = Var2, replacement = Var1)
top_var_replacements$rep_in_variable <- top_var_replacements$replacement %in%
  top_var_replacements$variable
```

For our explanatory variable, we have a classification problem. We have decided to make it a binary variable detailing whether a player is ever able to make it into the top 10 of an event.

# Modeling
To begin our modeling process, let's define our validation method: k-fold cross-validation. Let's also separate our data into folds for this process. Let's also preprocess a recipe for our data where we normalize our numeric predictors to eliminate issues with units

```{r preprocessing}
#Split dataset into training and testing partitions, using approximately 70% for training and 30% for testing
#set seed for repeatability
set.seed(777)
testing_set_rows <- sample(1:195,59)
pga_partitioned <- pga_data %>% 
  mutate(training_set = (!as.integer(row.names(pga_data)) %in%
                           testing_set_rows)) %>%
  .[,c(71,1:70)] %>% select(c(training_set,player,top_ten_finisher,top_var$var))

pga_train <- pga_partitioned %>% filter(training_set) %>% .[,-1]
pga_test <- pga_partitioned %>% filter(!training_set) %>% .[,-1]

set.seed(526)
cv_pga <- pga_train %>% vfold_cv(v = 5)

#create a recipe for TidyModels
pga_recipe <- pga_train %>%
  recipe(top_ten_finisher ~ .) %>%
  update_role(player, new_role = "id variable") %>%
  step_normalize(all_predictors()&all_numeric()) %>%
  step_dummy(all_predictors()&all_nominal())

pga_clean <- pga_recipe %>% prep() %>% juice()
```

To start, let's set up several different models: Maximum Likelihood Estimation (Logistic Regression),

```{r log_model}
### Logistic regression (Elasticnet)

#Create Elasticnet logistic regression model parsnip object
log_model <- logistic_reg(penalty = tune(),mixture = tune()) %>%
  set_engine("glmnet")

#Create EN Log reg workflow
log_workflow <- workflow() %>% add_model(log_model) %>%
  add_recipe(pga_recipe)


#Plotting strokes gained off the tee vs. drive average distanc
sg_drive <- ggplot(pga_data, aes(SG:OTT, AVG_Driving_DISTANCE)) + geom_point()

#tune workflow bassed on cv
log_cv <- log_workflow %>%
  tune_grid(
    cv_pga,metrics = metric_set(accuracy),
    grid = grid_regular(mixture(), penalty(), levels = 3)
  )


#select the best model
log_selected <- log_workflow %>%
  finalize_workflow(select_best(log_cv, 'accuracy'))

#fit model to training data
log_fit <- log_selected %>% fit(data = pga_train)

#predict results for testing data
log_predictions <- log_fit %>% 
  predict(new_data = pga_test, type = "class") %>% unlist()
```

```{r dt_model}
### Decision Tree

#Create parsnip model
tree_model <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = 5
) %>%
  set_mode("classification") %>%
  set_engine("rpart")

#create workflow
tree_workflow <- workflow() %>%
  add_model(tree_model) %>%
  add_recipe(pga_recipe)

#tune workflow
tree_cv <- tree_workflow %>%
  tune_grid(
    cv_pga,
    grid = expand_grid(
      cost_complexity = seq(0,.15, by = .05),
      tree_depth = c(1,5,10)
    ),
    metrics = metric_set(accuracy, roc_auc)
  )

#select best model
tree_selected <- tree_workflow %>%
  finalize_workflow(select_best(tree_cv, metric = "accuracy"))

#fit to training set
tree_fit <- tree_selected %>% fit(data = pga_train)

#predict testing set
tree_predictions <- tree_fit %>% predict(new_data = pga_test) %>% unlist()
```

```{r}
### Random Forest Model

set.seed(9870)

pga_split <- initial_split(pga_data, prop=0.8)
pga_train2 <- pga_split %>% training()
pga_test2 <- pga_split %>% testing()

cv_pga <- pga_train %>% vfold_cv(v = 5)

#create a NEW recipe for our random forest involving the split data 
pga_recipe2 <- pga_train %>%
  recipe(top_ten_finisher ~ .) %>%
  update_role(player, new_role = "id variable") %>%
  step_normalize(all_predictors()&all_numeric()) %>%
  step_dummy(all_predictors()&all_nominal())

pga_clean2 <- pga_recipe %>% prep() %>% juice()

# The recipe has been prepped and juiced; tune the hyperparameters

tune_spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger")

#Create workflow

tune_wf <- workflow() %>%
  add_recipe(pga_recipe2) %>%
  add_model(tune_spec)

#Train the hyperparameters 

set.seed(789)
pga_folds <- vfold_cv(pga_train2)

# Use parallel processing to speed things up

doParallel::registerDoParallel()

# Train many models, see which works best.

set.seed(1011)
tune_res <- tune_grid(
  tune_wf,
  resamples = pga_folds,
  grid = 20
)

tune_res
```

```{r}

tune_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>% ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")

# Set ranges for hyperparameters we want to try


```
```{r}
#Set ranges for hyperparameters we want to try

rf_grid <- grid_regular(
  mtry(range = c(2, 8)),
  min_n(range = c(30, 40)),
  levels = 5
)

rf_grid
```
```{r}
#Tune again, this time in a more targeted way

set.seed(456)
regular_res <- tune_grid(
  tune_wf,
  resamples = pga_folds,
  grid = rf_grid
)

regular_res
```
```{r}
#Check our new results

regular_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "AUC")
```
```{r}
#Choose the best model to use

best_auc <- select_best(regular_res, "roc_auc")

final_rf <- finalize_model(
  tune_spec,
  best_auc
)

final_rf

#Visually, this aligns with the highest rate of correct identification of true positives and true negatives.
```
```{r}
#Let's explore variable inportance to the model now.

library(vip)

final_rf %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(top_ten_finisher ~ .,
    data = pga_clean
  ) %>%
  vip(geom = "point")


# It seem strokes gained approaching the green as well as number of bunkers hit and number of par saves are the three most important variables besides the obvious point standings.
```
```{r}
#Create a final workflow

final_wf <- workflow() %>%
  add_recipe(pga_recipe2) %>%
  add_model(final_rf)

final_res <- final_wf %>%
  last_fit(pga_split)

final_res %>%
  collect_metrics()

```



